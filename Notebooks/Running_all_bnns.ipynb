{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "from datetime import datetime as DT\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from theano import shared\n",
    "from pymc_models import PyMCModel\n",
    "from pymc_models import bayes_nn_model_ARD_1HL_halfCauchy_hyperpriors as bnn_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../PickleJar/DataSets/AphiTrainTestSplitDataSets.pkl', 'rb') as fb:\n",
    "    datadict = pickle.load(fb)\n",
    "X_s_train = datadict['x_train_s']\n",
    "y_train = datadict['y_train']\n",
    "X_s_test = datadict['x_test_s']\n",
    "y_test = datadict['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-11 15:06:19.946 | INFO     | __main__:<module>:11 - processing aphi411\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (4 chains in 1 job)\n",
      "NUTS: [sd, bias_o, bias_1, wts_1_o_intrmd, wts_i_1_intrmd, hyp_bias_1_sd, hyp_w_1_out_sd, hyp_w_i_1_sd]\n",
      "100%|██████████| 12000/12000 [57:32<00:00,  3.43it/s]\n",
      "100%|██████████| 12000/12000 [57:52<00:00,  3.46it/s]\n",
      "100%|██████████| 12000/12000 [57:52<00:00,  3.46it/s]\n",
      "100%|██████████| 12000/12000 [58:03<00:00,  3.43it/s]\n",
      "There were 26 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 44 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 56 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 81 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "100%|██████████| 8000/8000 [00:49<00:00, 175.70it/s]\n",
      "/accounts/ekarakoy/anaconda3/envs/bayesian_toa_project/lib/python3.7/site-packages/pymc3/stats.py:167: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  return np.stack(logp)\n",
      "/accounts/ekarakoy/anaconda3/envs/bayesian_toa_project/lib/python3.7/site-packages/pymc3/stats.py:218: UserWarning: For one or more samples the posterior variance of the\n",
      "        log predictive densities exceeds 0.4. This could be indication of\n",
      "        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\")\n",
      "/accounts/ekarakoy/anaconda3/envs/bayesian_toa_project/lib/python3.7/site-packages/pymc3/stats.py:299: UserWarning: Estimated shape parameter of Pareto distribution is\n",
      "        greater than 0.7 for one or more samples.\n",
      "        You should consider using a more robust model, this is because\n",
      "        importance sampling is less likely to work well if the marginal\n",
      "        posterior and LOO posterior are very different. This is more likely to\n",
      "        happen with a non-robust model and highly influential observations.\n",
      "  happen with a non-robust model and highly influential observations.\"\"\")\n",
      "100%|██████████| 8000/8000 [00:48<00:00, 163.69it/s]\n",
      "2019-03-11 18:59:33.266 | INFO     | __main__:<module>:11 - processing aphi443\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (4 chains in 1 job)\n",
      "NUTS: [sd, bias_o, bias_1, wts_1_o_intrmd, wts_i_1_intrmd, hyp_bias_1_sd, hyp_w_1_out_sd, hyp_w_i_1_sd]\n",
      "100%|██████████| 12000/12000 [57:35<00:00,  3.39it/s]\n",
      "100%|██████████| 12000/12000 [57:59<00:00,  3.41it/s]\n",
      "100%|██████████| 12000/12000 [58:27<00:00,  3.45it/s]\n",
      "100%|██████████| 12000/12000 [58:27<00:00,  3.40it/s]\n",
      "There were 39 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 80 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 139 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 209 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "100%|██████████| 8000/8000 [00:49<00:00, 161.14it/s]\n",
      "100%|██████████| 8000/8000 [00:49<00:00, 162.65it/s]\n",
      "2019-03-11 22:53:52.424 | INFO     | __main__:<module>:11 - processing aphi489\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (4 chains in 1 job)\n",
      "NUTS: [sd, bias_o, bias_1, wts_1_o_intrmd, wts_i_1_intrmd, hyp_bias_1_sd, hyp_w_1_out_sd, hyp_w_i_1_sd]\n",
      "100%|██████████| 12000/12000 [57:20<00:00,  3.31it/s]\n",
      "100%|██████████| 12000/12000 [58:41<00:00,  3.42it/s]\n",
      "100%|██████████| 12000/12000 [58:20<00:00,  3.41it/s]\n",
      "100%|██████████| 12000/12000 [58:49<00:00,  3.36it/s]\n",
      "There were 85 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8693918381399415, but should be close to 0.95. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 113 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 144 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 177 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "100%|██████████| 8000/8000 [00:50<00:00, 159.80it/s]\n",
      "100%|██████████| 8000/8000 [00:49<00:00, 161.15it/s]\n",
      "2019-03-12 02:48:55.740 | INFO     | __main__:<module>:11 - processing aphi510\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (4 chains in 1 job)\n",
      "NUTS: [sd, bias_o, bias_1, wts_1_o_intrmd, wts_i_1_intrmd, hyp_bias_1_sd, hyp_w_1_out_sd, hyp_w_i_1_sd]\n",
      "100%|██████████| 12000/12000 [57:29<00:00,  3.38it/s]\n",
      "100%|██████████| 12000/12000 [58:03<00:00,  3.43it/s]\n",
      "100%|██████████| 12000/12000 [58:12<00:00,  3.41it/s]\n",
      "100%|██████████| 12000/12000 [58:16<00:00,  3.40it/s]\n",
      "There were 4 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 68 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 104 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 121 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "100%|██████████| 8000/8000 [00:50<00:00, 158.76it/s]\n",
      "100%|██████████| 8000/8000 [00:49<00:00, 161.54it/s]\n",
      "2019-03-12 06:42:47.076 | INFO     | __main__:<module>:11 - processing aphi555\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (4 chains in 1 job)\n",
      "NUTS: [sd, bias_o, bias_1, wts_1_o_intrmd, wts_i_1_intrmd, hyp_bias_1_sd, hyp_w_1_out_sd, hyp_w_i_1_sd]\n",
      "100%|██████████| 12000/12000 [56:24<00:00,  3.40it/s]\n",
      "100%|██████████| 12000/12000 [57:53<00:00,  3.39it/s]\n",
      "100%|██████████| 12000/12000 [57:46<00:00,  3.46it/s]\n",
      "100%|██████████| 12000/12000 [58:17<00:00,  3.52it/s]\n",
      "There were 86 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were 133 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 203 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 297 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n",
      "100%|██████████| 8000/8000 [00:51<00:00, 154.79it/s]\n",
      "100%|██████████| 8000/8000 [00:50<00:00, 156.92it/s]\n",
      "2019-03-12 10:35:00.745 | INFO     | __main__:<module>:11 - processing aphi670\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (4 chains in 1 job)\n",
      "NUTS: [sd, bias_o, bias_1, wts_1_o_intrmd, wts_i_1_intrmd, hyp_bias_1_sd, hyp_w_1_out_sd, hyp_w_i_1_sd]\n",
      "100%|██████████| 12000/12000 [59:12<00:00,  3.38it/s] \n",
      "100%|██████████| 12000/12000 [59:16<00:00,  3.66it/s] \n",
      "100%|██████████| 12000/12000 [59:23<00:00,  2.96it/s]\n",
      "100%|██████████| 12000/12000 [59:49<00:00,  3.29it/s] \n",
      "There were 27 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 132 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 184 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "There were 225 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.2 for some parameters.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "100%|██████████| 8000/8000 [00:54<00:00, 147.58it/s]\n",
      "100%|██████████| 8000/8000 [00:53<00:00, 148.99it/s]\n"
     ]
    }
   ],
   "source": [
    "bands = [411, 443, 489, 510, 555, 670]\n",
    "# create band-keyed dictionary to contain models\n",
    "model_dict=dict.fromkeys(bands)\n",
    "\n",
    "# create theano shared variable\n",
    "X_shared = shared(X_s_train.values)\n",
    "y_shared = shared(y_train['log10_aphy%d' % bands[0]].values)\n",
    "# Fitting aphi411 model:\n",
    "# Instantiate PyMC3 model with bnn likelihood\n",
    "for band in bands:\n",
    "    logger.info(\"processing aphi{band}\", band=band)\n",
    "    X_shared.set_value(X_s_train.values)\n",
    "    y_shared.set_value(y_train['log10_aphy%d' % band].values)\n",
    "    bnn = PyMCModel(bnn_mdl, X_shared, y_shared )\n",
    "    bnn.model.name = 'bnn_aphy%d' %band\n",
    "    bnn.fit(n_samples=2000, cores=1, chains=4, tune=10000,\n",
    "                nuts_kwargs=dict(target_accept=0.95))\n",
    "    ppc_train_ = bnn.predict(likelihood_name='likelihood')\n",
    "    waic_train = bnn.get_waic()\n",
    "    loo_train = bnn.get_loo()\n",
    "    model_train = deepcopy(bnn.model)\n",
    "    trace = deepcopy(bnn.trace_)\n",
    "    run_dict = dict(model_train=model_train, trace=trace,\n",
    "                    ppc_train=ppc_train_, loo_train=loo_train, waic_train=waic_train)\n",
    "    X_shared.set_value(X_s_test.values)\n",
    "    y_shared.set_value(y_test['log10_aphy%d' % band].values)\n",
    "    model_test = deepcopy(bnn.model)\n",
    "    ppc_test_ = bnn.predict(likelihood_name='likelihood')\n",
    "    waic_test = bnn.get_waic()\n",
    "    loo_test = bnn.get_loo()\n",
    "    run_dict.update(dict(model_test=model_test, ppc_test=ppc_test_,\n",
    "                         waic_test=waic_test, loo_test=loo_test))\n",
    "    model_dict[band] = run_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../PickleJar/Results/bnn_model_dict_%s.pkl' %DT.now(), 'wb') as fb:\n",
    "        pickle.dump(model_dict, fb, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bayesian_toa_project] *",
   "language": "python",
   "name": "conda-env-bayesian_toa_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
